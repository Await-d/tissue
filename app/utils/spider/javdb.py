import re
import time
import logging
import re
from datetime import datetime
from random import randint
from urllib.parse import urljoin, urlparse
from lxml import etree
from app.schema import VideoDetail, VideoActor, VideoDownload, VideoPreviewItem, VideoPreview
from app.schema.home import JavDBRanking
from app.utils.spider.spider import Spider
from app.utils.spider.spider_exception import SpiderException

# è·å–logger
logger = logging.getLogger('spider')

class JavdbSpider(Spider):
    host = "https://javdb.com"
    name = 'JavDB'
    downloadable = True
    avatar_host = 'https://c0.jdbstatic.com/avatars/'

    # å€™é€‰é•œåƒåŸŸååˆ—è¡¨ï¼ˆå¯æ‰©å±•/ä¸ç«™ç‚¹ç®¡ç†é…ç½®é…åˆä½¿ç”¨ï¼‰
    mirror_hosts = [
        "https://javdb.com",
        "https://javdb36.com",
        "https://javdb37.com",
        "https://javdb47.com",
    ]

    def __init__(self):
        # åˆï¿½ï¿½ï¿½åŒ–åŸºç¡€ä¼šè¯é…ç½®
        super().__init__()
        # åŠ¨æ€é€‰æ‹©å¯ç”¨åŸŸåï¼ˆè¢«å°æˆ–ä¸å¯è¾¾æ—¶è‡ªåŠ¨åˆ‡æ¢ï¼‰
        try:
            self._select_best_host()
        except Exception as e:
            logger.warning(f"é€‰æ‹©JavDBå¯ç”¨åŸŸåå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ {self.host}: {e}")

    def _cookie_domain(self) -> str:
        netloc = urlparse(self.host).netloc
        return f".{netloc}"

    def _set_age_cookies(self):
        """ä¸ºå½“å‰hostè®¾ç½®18+ä¸è¯­è¨€Cookie"""
        try:
            dom = self._cookie_domain()
            self.session.cookies.set('over18', '1', domain=dom)
            self.session.cookies.set('locale', 'zh', domain=dom)
        except Exception as e:
            logger.debug(f"è®¾ç½®å¹´é¾„éªŒè¯Cookieå¤±è´¥: {e}")

    def _select_best_host(self):
        """å°è¯•é•œåƒåŸŸåï¼Œé€‰æ‹©å¯ç”¨çš„host"""
        # å»é‡å¹¶ä¿æŒé¡ºåºï¼šä¼˜å…ˆä½¿ç”¨å†…ç½®é•œåƒåˆ—è¡¨ï¼Œå†åŒ…å«å½“å‰é»˜è®¤host
        candidates = list(dict.fromkeys(self.mirror_hosts + [self.host]))
        test_paths = ["/videos", "/rankings/movies?p=weekly&t=censored", "/"]
        headers = {
            "User-Agent": self.session.headers.get("User-Agent", ""),
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        }

        for base in candidates:
            for path in test_paths:
                try:
                    resp = self.session.get(urljoin(base, path), headers=headers)
                    # çŠ¶æ€ç ä¸º200å³è®¤ä¸ºå¯ç”¨ï¼ˆé¿å…è¯¯æŠŠé˜²ç«å¢™/æŒ‘æˆ˜é¡µå½“ä½œå°ç¦ï¼‰
                    if resp.status_code == 200:
                        self.host = base
                        # åŒæ­¥æ›´æ–°Refererï¼Œé¿å…éƒ¨åˆ†é¡µé¢æ ¡éªŒå¤±è´¥
                        self.session.headers["Referer"] = self.host
                        self._set_age_cookies()
                        logger.info(f"JavDBå¯ç”¨åŸŸå: {self.host}")
                        return
                except Exception:
                    continue

        # è‹¥éƒ½ä¸å¯ç”¨ï¼Œä»è®¾ç½®åŸºäºç°æœ‰hostçš„cookie
        logger.warning("æœªèƒ½è‡ªåŠ¨ç¡®è®¤JavDBå¯ç”¨åŸŸåï¼Œå°†ç»§ç»­ä½¿ç”¨é»˜è®¤host")
        self._set_age_cookies()

    def _rebuild_url_for_current_host(self, absolute_or_relative_url: str) -> str:
        try:
            parsed = urlparse(absolute_or_relative_url)
            if not parsed.scheme:
                return urljoin(self.host, absolute_or_relative_url)
            current = urlparse(self.host)
            if parsed.netloc != current.netloc:
                path_with_query = parsed.path
                if parsed.query:
                    path_with_query += f"?{parsed.query}"
                return urljoin(self.host, path_with_query)
            return absolute_or_relative_url
        except Exception:
            return urljoin(self.host, absolute_or_relative_url)

    def _is_banned_response(self, resp) -> bool:
        try:
            if resp.status_code in (401, 403, 429, 503):
                return True
            content_lower = resp.content.lower() if isinstance(resp.content, (bytes, bytearray)) else b""
            markers = [b"banned your access", b"access denied", b"captcha", b"forbidden", b"just a moment"]
            return any(m in content_lower for m in markers)
        except Exception:
            return False

    def _get(self, url: str, headers=None):
        target = self._rebuild_url_for_current_host(url)
        resp = self.session.get(target, headers=headers) if headers else self.session.get(target)
        if self._is_banned_response(resp):
            logger.warning("æ£€æµ‹åˆ°è¢«å°ç¦/é£æ§ï¼Œå°è¯•åˆ‡æ¢é•œåƒåŸŸååé‡è¯•")
            try:
                self._select_best_host()
            except Exception:
                pass
            target = self._rebuild_url_for_current_host(url)
            resp = self.session.get(target, headers=headers) if headers else self.session.get(target)
        return resp

    def get_info(self, num: str, url: str = None, include_downloads=False, include_previews=False):

        searched = False

        if url is None:
            url = self.search(num)
            searched = True
        else:
            # ç¡®ä¿URLæ˜¯å®Œæ•´çš„ç»å¯¹URL
            if not url.startswith('http'):
                url = urljoin(self.host, url)

        if not url:
            raise SpiderException('æœªæ‰¾åˆ°ç•ªå·')
        else:
            if searched:
                time.sleep(randint(1, 3))

        meta = VideoDetail()
        meta.num = num

        response = self._get(url)
        html = etree.HTML(response.content, parser=etree.HTMLParser(encoding='utf-8'))

        title_element = html.xpath("//strong[@class='current-title']")
        if title_element:
            title = title_element[0].text.strip()
            meta.title = f'{num.upper()} {title}'

        premiered_element = html.xpath("//strong[text()='æ—¥æœŸ:']/../span")
        if premiered_element:
            meta.premiered = premiered_element[0].text

        runtime_element = html.xpath("//strong[text()='æ™‚é•·:']/../span")
        if runtime_element:
            runtime = runtime_element[0].text
            runtime = runtime.replace(" åˆ†é¾", "")
            meta.runtime = runtime

        director_element = html.xpath("//strong[text()='å°æ¼”:']/../span/a")
        if director_element:
            director = director_element[0].text
            meta.director = director

        studio_element = html.xpath("//strong[text()='ç‰‡å•†:']/../span/a")
        if studio_element:
            studio = studio_element[0].text
            meta.studio = studio

        publisher_element = html.xpath("//strong[text()='ç™¼è¡Œ:']/../span/a")
        if publisher_element:
            publisher = publisher_element[0].text
            meta.publisher = publisher

        series_element = html.xpath("//strong[text()='ç³»åˆ—:']/../span/a")
        if series_element:
            series = series_element[0].text
            meta.series = series

        tag_elements = html.xpath("//a[contains(@href,'/tags?')]")
        if tag_elements:
            tags = [tag.text for tag in tag_elements]
            meta.tags = tags

        actor_elements = html.xpath("//strong[@class='symbol female']")
        if actor_elements:
            actors = []
            for element in actor_elements:
                actor_element = element.xpath('./preceding-sibling::a[1]')[0]
                actor_url = actor_element.get('href')
                actor_code = actor_url.split("/")[-1]
                actor_avatar = urljoin(self.avatar_host, f'{actor_code[0:2].lower()}/{actor_code}.jpg')
                actor = VideoActor(name=actor_element.text, thumb=actor_avatar)
                actors.append(actor)
            meta.actors = actors

        cover_element = html.xpath("//img[@class='video-cover']")
        if cover_element:
            meta.cover = cover_element[0].get("src")

        score_elements = html.xpath("//span[@class='score-stars']/../text()")
        if score_elements:
            score_text = str(score_elements[0])
            pattern_result = re.search(r"(\d+\.\d+)åˆ†", score_text)
            score = pattern_result.group(1)
            meta.rating = score

        # è·å–è¯„è®ºæ•°
        comments_elements = html.xpath("//a[contains(@href,'/reviews?')]/text()")
        if comments_elements:
            comments_text = comments_elements[0]
            comments_match = re.search(r"(\d+)", comments_text)
            if comments_match:
                meta.comments_count = int(comments_match.group(1))

        meta.website.append(url)

        if include_downloads:
            meta.downloads = self.get_downloads(url, html)

        if include_previews:
            meta.previews = self.get_previews(html)

        return meta

    def search(self, num: str):
        url = urljoin(self.host, f"/search?q={num}&f=all")
        response = self._get(url)

        html = etree.HTML(response.content)
        matched_elements = html.xpath(fr"//div[@class='video-title']/strong")
        # è®°å½•æœç´¢ç»“æœç”¨äºè°ƒè¯•
        logger.debug(f"æœç´¢ {num} æ‰¾åˆ° {len(matched_elements)} ä¸ªç»“æœ")
        
        # ç²¾ç¡®åŒ¹é…
        for matched_element in matched_elements:
            element_text = matched_element.text
            if element_text and element_text.strip().lower() == num.lower():
                code = matched_element.xpath('./../..')[0].get('href')
                logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„ç•ªå·: {element_text} -> {code}")
                return urljoin(self.host, code)
        
        # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå»æ‰æ¨ªæ ç­‰ï¼‰
        num_normalized = num.lower().replace('-', '').replace('_', '')
        for matched_element in matched_elements:
            element_text = matched_element.text
            if element_text:
                element_normalized = element_text.strip().lower().replace('-', '').replace('_', '')
                if element_normalized == num_normalized:
                    code = matched_element.xpath('./../..')[0].get('href')
                    logger.info(f"æ‰¾åˆ°æ¨¡ç³ŠåŒ¹é…çš„ç•ªå·: {element_text} -> {code}")
                    return urljoin(self.host, code)
        
        # è®°å½•å‰å‡ ä¸ªæœç´¢ç»“æœç”¨äºè°ƒè¯•
        if matched_elements:
            logger.debug(f"æœç´¢ç»“æœå‰5ä¸ªç•ªå·:")
            for i, elem in enumerate(matched_elements[:5]):
                if elem.text:
                    logger.debug(f"  {i+1}. {elem.text}")
        else:
            logger.warning(f"æœç´¢ {num} æœªæ‰¾åˆ°ä»»ä½•ç»“æœ")
        
        return None

    def get_previews(self, html: etree.HTML):
        result = []

        videos = html.xpath("//div[contains(@class,'preview-images')]/a[@class='preview-video-container']")
        for video in videos:
            thumb = video.xpath('./img')[0]
            video = html.xpath(f"//video[@id='{video.get('href')[1:]}']/source")[0]
            preview = VideoPreviewItem(type='video', thumb=thumb.get('src'), url=video.get('src'))
            result.append(preview)

        images = html.xpath("//div[contains(@class,'preview-images')]/a[@class='tile-item']")
        for image in images:
            thumb = image.xpath('./img')[0]
            preview = VideoPreviewItem(type='image', thumb=thumb.get('src'), url=image.get('href'))
            result.append(preview)

        return [VideoPreview(website=self.name, items=result)]

    def get_trending_videos(self, page: int = 1, time_range: str = "week"):
        """è·å–çƒ­é—¨è§†é¢‘åˆ—è¡¨"""
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
            delay = randint(3, 8)
            logger.info(f"è·å–çƒ­é—¨è§†é¢‘åˆ—è¡¨å‰ç­‰å¾… {delay} ç§’...")
            time.sleep(delay)
            
            # æ„é€ çƒ­é—¨é¡µé¢URL
            url = urljoin(self.host, f"/rankings/videos?t={time_range}&page={page}")
            logger.info(f"è·å–çƒ­é—¨è§†é¢‘åˆ—è¡¨: {url}")
            
            # æ„å»ºè¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Cache-Control": "max-age=0",
                "Connection": "keep-alive",
                "Referer": self.host
            }
            
            response = self._get(url, headers=headers)
            html = etree.HTML(response.content, parser=etree.HTMLParser(encoding='utf-8'))
            
            videos = []
            video_elements = html.xpath("//div[@class='item']")
            
            for element in video_elements:
                try:
                    # è·å–ç•ªå·
                    title_element = element.xpath(".//div[@class='video-title']/strong")
                    if not title_element:
                        continue
                    
                    num = title_element[0].text.strip()
                    
                    # è·å–é“¾æ¥
                    link_element = element.xpath(".//a[@class='box']")
                    if not link_element:
                        continue
                    
                    video_url = urljoin(self.host, link_element[0].get('href'))
                    
                    # è·å–å°é¢
                    cover_element = element.xpath(".//img")
                    cover = cover_element[0].get('src') if cover_element else None
                    
                    # è·å–è¯„åˆ†
                    rating_element = element.xpath(".//span[@class='score']")
                    rating = None
                    if rating_element:
                        rating_text = rating_element[0].text
                        rating_match = re.search(r"(\d+\.\d+)", rating_text)
                        if rating_match:
                            rating = float(rating_match.group(1))
                    
                    video_info = {
                        'num': num,
                        'title': f"{num} {title_element[0].tail or ''}".strip(),
                        'url': video_url,
                        'cover': cover,
                        'rating': rating,
                        'website': self.name
                    }
                    
                    videos.append(video_info)
                    
                except Exception as e:
                    logger.warning(f"è§£æè§†é¢‘ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            return videos
            
        except Exception as e:
            logger.error(f"è·å–çƒ­é—¨è§†é¢‘åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
            return []

    def get_latest_videos(self, page: int = 1, date_range: int = 7):
        """è·å–æœ€æ–°è§†é¢‘åˆ—è¡¨"""
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
            delay = randint(3, 8)
            logger.info(f"è·å–æœ€æ–°è§†é¢‘åˆ—è¡¨å‰ç­‰å¾… {delay} ç§’...")
            time.sleep(delay)
            
            # æ„é€ æœ€æ–°é¡µé¢URL  
            url = urljoin(self.host, f"/videos?page={page}")
            logger.info(f"è·å–æœ€æ–°è§†é¢‘åˆ—è¡¨: {url}")
            
            # æ„å»ºè¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Cache-Control": "max-age=0",
                "Connection": "keep-alive",
                "Referer": self.host
            }
            
            response = self._get(url, headers=headers)
            html = etree.HTML(response.content, parser=etree.HTMLParser(encoding='utf-8'))
            
            videos = []
            video_elements = html.xpath("//div[@class='item']")
            
            for element in video_elements:
                try:
                    # è·å–ç•ªå·
                    title_element = element.xpath(".//div[@class='video-title']/strong")
                    if not title_element:
                        continue
                    
                    num = title_element[0].text.strip()
                    
                    # è·å–é“¾æ¥
                    link_element = element.xpath(".//a[@class='box']")
                    if not link_element:
                        continue
                    
                    video_url = urljoin(self.host, link_element[0].get('href'))
                    
                    # è·å–å‘å¸ƒæ—¥æœŸ
                    date_element = element.xpath(".//div[@class='meta']/text()")
                    publish_date = None
                    if date_element:
                        date_text = date_element[0].strip()
                        try:
                            publish_date = datetime.strptime(date_text, "%Y-%m-%d").date()
                        except:
                            pass
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…
                    if publish_date and date_range > 0:
                        days_ago = (datetime.now().date() - publish_date).days
                        if days_ago > date_range:
                            continue
                    
                    # è·å–å°é¢
                    cover_element = element.xpath(".//img")
                    cover = cover_element[0].get('src') if cover_element else None
                    
                    # è·å–è¯„åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
                    rating_element = element.xpath(".//span[@class='score']")
                    rating = None
                    if rating_element:
                        rating_text = rating_element[0].text
                        rating_match = re.search(r"(\d+\.\d+)", rating_text)
                        if rating_match:
                            rating = float(rating_match.group(1))
                    
                    video_info = {
                        'num': num,
                        'title': f"{num} {title_element[0].tail or ''}".strip(),
                        'url': video_url,
                        'cover': cover,
                        'rating': rating,
                        'comments': 0,  # æœ€æ–°è§†é¢‘é¡µé¢æ²¡æœ‰è¯„è®ºæ•°ä¿¡æ¯ï¼Œè®¾ç½®ä¸º0
                        'comments_count': 0,  # æœ€æ–°è§†é¢‘é¡µé¢æ²¡æœ‰è¯„è®ºæ•°ä¿¡æ¯ï¼Œè®¾ç½®ä¸º0
                        'publish_date': publish_date,
                        'website': self.name
                    }
                    
                    videos.append(video_info)
                    
                except Exception as e:
                    logger.warning(f"è§£æè§†é¢‘ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            return videos
            
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°è§†é¢‘åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
            return []



    def get_comments_count(self, url: str):
        """è·å–è§†é¢‘è¯„è®ºæ•°"""
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
            delay = randint(2, 5)
            logger.debug(f"è·å–è¯„è®ºæ•°å‰ç­‰å¾… {delay} ç§’...")
            time.sleep(delay)
            
            # æ„å»ºè¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Cache-Control": "max-age=0",
                "Connection": "keep-alive",
                "Referer": self.host
            }
            
            response = self._get(url, headers=headers)
            html = etree.HTML(response.content, parser=etree.HTMLParser(encoding='utf-8'))
            
            comments_elements = html.xpath("//a[contains(@href,'/reviews?')]/text()")
            if comments_elements:
                comments_text = comments_elements[0]
                comments_match = re.search(r"(\d+)", comments_text)
                if comments_match:
                    return int(comments_match.group(1))
            
            return 0
            
        except Exception as e:
            logger.error(f"è·å–è¯„è®ºæ•°æ—¶å‡ºé”™: {str(e)}")
            return 0

    def get_downloads(self, url: str, html: etree.HTML):
        result = []
        table = html.xpath("//div[@id='magnets-content']/div")
        for item in table:
            download = VideoDownload()

            parts = item.xpath("./div[1]/a")[0]
            download.website = self.name
            download.url = url
            download.name = parts[0].text.strip()
            download.magnet = parts.get('href')

            name = parts.xpath("./span[1]")
            if name:
                if 'æ— ç ' in name[0].text or 'ç ´è§£' in name[0].text:
                    download.is_uncensored = True

            size = parts.xpath("./span[2]")
            if size:
                download.size = size[0].text.split(',')[0].strip()

            for tag in parts.xpath('./div[@class="tags"]/span'):
                if tag.text == 'é«˜æ¸…':
                    download.is_hd = True
                if tag.text == 'å­—å¹•':
                    download.is_zh = True

            publish_date = item.xpath(".//span[@class='time']")
            if publish_date:
                download.publish_date = datetime.strptime(publish_date[0].text.strip(), "%Y-%m-%d").date()

            result.append(download)
        return result

    def get_ranking(self, video_type: str, cycle: str):
        url = urljoin(self.host, f'/rankings/movies?p={cycle}&t={video_type}')
        response = self._get(url)
        html = etree.HTML(response.content, parser=etree.HTMLParser(encoding='utf-8'))

        result = []

        videos = html.xpath('//div[contains(@class, "movie-list")]/div[@class="item"]/a')
        for video in videos:
            ranking = JavDBRanking()
            ranking.cover = video.xpath('./div[contains(@class, "cover")]/img')[0].get('src')
            ranking.title = video.get('title')
            ranking.num = video.xpath('./div[@class="video-title"]/strong')[0].text
            ranking.publish_date = datetime.strptime(video.xpath('./div[@class="meta"]')[0].text.strip(),
                                                     "%Y-%m-%d").date()

            rank_str = video.xpath('./div[@class="score"]/span/text()')[0].strip()
            rank_matched = re.match('(.+?)åˆ†, ç”±(.+?)äººè©•åƒ¹', rank_str)
            ranking.rank = float(rank_matched.group(1))
            ranking.rank_count = int(rank_matched.group(2))

            ranking.url = urljoin(self.host, video.get('href'))

            tag_str = video.xpath('./div[contains(@class, "tags")]/span/text()')[0]
            ranking.isZh = ('ä¸­å­—' in tag_str)

            result.append(ranking)
        return result

    def get_ranking_with_details(self, video_type: str, cycle: str, max_pages: int = 1):
        """è·å–æ’è¡Œæ¦œæ•°æ®ï¼ŒåŒ…å«è¯„åˆ†å’Œè¯„è®ºä¿¡æ¯ï¼Œç”¨äºæ™ºèƒ½ä¸‹è½½è§„åˆ™"""
        try:
            # æ„é€ æ’è¡Œæ¦œURL - æ’è¡Œæ¦œé¡µé¢ä¸éœ€è¦åˆ†é¡µï¼Œä¸€æ¬¡è¿”å›å…¨éƒ¨æ•°æ®
            if video_type == 'uncensored':
                # æ— ç æ’è¡Œæ¦œä½¿ç”¨moviesè·¯å¾„ï¼Œé€šè¿‡tå‚æ•°æŒ‡å®šuncensored
                url = urljoin(self.host, f"/rankings/movies?p={cycle}&t=uncensored")
                page_type = 'uncensored_ranking'
            else:
                # æœ‰ç æ’è¡Œæ¦œåŒæ ·ä½¿ç”¨moviesè·¯å¾„ï¼Œé€šè¿‡tå‚æ•°æŒ‡å®šcensored
                url = urljoin(self.host, f"/rankings/movies?p={cycle}&t=censored")
                page_type = 'censored_ranking'
                
            logger.info(f"è·å–æ’è¡Œæ¦œé¡µé¢: {url} (ç±»å‹: {page_type})")
                
            # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
            delay = randint(3, 8)
            logger.info(f"ç­‰å¾… {delay} ç§’...")
            time.sleep(delay)
            
            # æ„å»ºè¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Cache-Control": "max-age=0",
                "Connection": "keep-alive",
                "Referer": self.host
            }
            
            # è®¾ç½®å¹´é¾„éªŒè¯Cookieç»•è¿‡æˆäººå†…å®¹ç¡®è®¤ï¼ˆæ ¹æ®å½“å‰hoståŠ¨æ€è®¾ç½®ï¼‰
            self._set_age_cookies()
            
            response = self._get(url, headers=headers)
            html = etree.HTML(response.content, parser=etree.HTMLParser(encoding='utf-8'))
            
            # ä¿å­˜é¡µé¢HTMLç”¨äºè°ƒè¯•
            debug_filename = f"javdb_{page_type}_debug.html"
            try:
                with open(debug_filename, "wb") as f:
                    f.write(response.content)
                logger.info(f"å·²ä¿å­˜è°ƒè¯•é¡µé¢åˆ° {debug_filename}")
            except:
                pass
            
            # æ ¹æ®é¡µé¢ç±»å‹ä½¿ç”¨ä¸åŒçš„è§£æç­–ç•¥
            if page_type == 'uncensored_ranking':
                videos = self._parse_uncensored_ranking_page(html, 1)
            else:
                videos = self._parse_censored_ranking_page(html, 1)
            
            if not videos:
                logger.warning(f"æœªè§£æåˆ°è§†é¢‘æ•°æ®")
                return []
            
            logger.info(f"æ’è¡Œæ¦œè§£æå®Œæˆï¼Œè·å–åˆ° {len(videos)} ä¸ªè§†é¢‘")
            
            # è¾“å‡ºå‰3ä¸ªè§†é¢‘çš„è¯¦ç»†ä¿¡æ¯ç”¨äºè°ƒè¯•
            for i, video in enumerate(videos[:3]):
                logger.info(f"è§†é¢‘ {i+1}: {video.get('num')} - è¯„åˆ†: {video.get('rating')} - è¯„è®º: {video.get('comments')} - é¡µé¢ç±»å‹: {video.get('page_type')}")
            
            return videos
            
        except Exception as e:
            logger.error(f"è·å–æ’è¡Œæ¦œæ•°æ®æ—¶å‡ºé”™: {str(e)}")
            import traceback
            logger.debug(traceback.format_exc())
            return []

    def _parse_censored_ranking_page(self, html, page):
        """è§£ææœ‰ç æ’è¡Œæ¦œé¡µé¢"""
        return self._parse_ranking_page(html, page, 'censored_ranking')
    
    def _parse_ranking_page(self, html, page, page_type):
        """ç»Ÿä¸€çš„æ’è¡Œæ¦œé¡µé¢è§£ææ–¹æ³• - æ ¹æ®å®é™…HTMLç»“æ„"""
        videos = []
        
        # æ’è¡Œæ¦œé¡µé¢ä½¿ç”¨movie-listç»“æ„
        video_elements = html.xpath("//div[@class='movie-list']//div[@class='item']")
        logger.info(f"{page_type}ç¬¬ {page} é¡µæ‰¾åˆ° {len(video_elements)} ä¸ªè§†é¢‘å…ƒç´ ")
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
        if not video_elements:
            alternative_selectors = [
                "//div[@class='item']",  # ç›´æ¥æŸ¥æ‰¾item
                "//div[contains(@class, 'movie-list')]//div[contains(@class, 'item')]",  # æ›´å®½æ¾çš„åŒ¹é…
                "//div[@class='grid-item']",  # å¯èƒ½çš„æ›¿ä»£classå
                "//div[contains(@class, 'video-item')]"  # å…¶ä»–å¯èƒ½çš„video item class
            ]
            
            for selector in alternative_selectors:
                video_elements = html.xpath(selector)
                if video_elements:
                    logger.info(f"ä½¿ç”¨å¤‡ç”¨é€‰æ‹©å™¨æ‰¾åˆ° {len(video_elements)} ä¸ªè§†é¢‘å…ƒç´ : {selector}")
                    break
                else:
                    logger.debug(f"å¤‡ç”¨é€‰æ‹©å™¨æ— ç»“æœ: {selector}")
        
        if not video_elements:
            # è¾“å‡ºé¡µé¢çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯ç”¨äºè°ƒè¯•
            page_title = html.xpath("//title/text()")
            if page_title:
                logger.warning(f"é¡µé¢æ ‡é¢˜: {page_title[0]}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¹´é¾„éªŒè¯æ¨¡æ€æ¡†
            modal_elements = html.xpath("//div[contains(@class, 'modal')]")
            if modal_elements:
                logger.warning(f"æ£€æµ‹åˆ° {len(modal_elements)} ä¸ªæ¨¡æ€æ¡†å…ƒç´ ï¼Œå¯èƒ½éœ€è¦å¹´é¾„éªŒè¯")
            
            logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘å…ƒç´ ï¼Œé¡µé¢å¯èƒ½ç»“æ„å·²å˜åŒ–æˆ–éœ€è¦é¢å¤–éªŒè¯")
        
        for element in video_elements:
            try:
                video_info = {}
                
                # è·å–ç•ªå· - ä»video-titleä¸‹çš„strongå…ƒç´ 
                title_element = element.xpath(".//div[@class='video-title']/strong")
                if not title_element or not title_element[0].text:
                    continue
                num = title_element[0].text.strip()
                video_info['num'] = num
                video_info['page_type'] = page_type
                
                # è·å–é“¾æ¥ - ä»a.boxå…ƒç´ 
                link_element = element.xpath(".//a[@class='box']")
                if link_element:
                    video_url = urljoin(self.host, link_element[0].get('href'))
                    video_info['url'] = video_url
                
                # è·å–æ ‡é¢˜ - video-titleçš„å®Œæ•´æ–‡æœ¬
                title_div = element.xpath(".//div[@class='video-title']")
                if title_div:
                    # æå–å®Œæ•´æ ‡é¢˜æ–‡æœ¬
                    full_title = ''.join(title_div[0].itertext()).strip()
                    video_info['title'] = full_title
                else:
                    video_info['title'] = num
                
                # è·å–å°é¢
                cover_element = element.xpath(".//img")
                if cover_element:
                    video_info['cover'] = cover_element[0].get('src')
                
                # è·å–è¯„åˆ†å’Œè¯„è®ºæ•° - ä»score divä¸­æå–
                score_element = element.xpath(".//div[@class='score']/span[@class='value']")
                rating = None
                comments = 0
                
                if score_element:
                    # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼ŒåŒ…æ‹¬åµŒå¥—å…ƒç´ çš„æ–‡æœ¬
                    score_text = ''.join(score_element[0].itertext()).strip()
                    logger.info(f"åŸå§‹è¯„åˆ†æ–‡æœ¬: '{score_text}'")
                    
                    # æå–è¯„åˆ†ï¼šæ ¼å¼å¦‚ "4.54åˆ†, ç”±346äººè©•åƒ¹"
                    rating_match = re.search(r"(\d+\.\d+)åˆ†", score_text)
                    if rating_match:
                        rating = float(rating_match.group(1))
                        logger.info(f"æˆåŠŸæå–è¯„åˆ†: {rating}")
                    
                    # æå–è¯„è®ºæ•°ï¼šæ ¼å¼å¦‚ "ç”±346äººè©•åƒ¹"
                    comment_match = re.search(r"ç”±(\d+)äººè©•åƒ¹", score_text)
                    if comment_match:
                        comments = int(comment_match.group(1))
                        logger.info(f"æˆåŠŸæå–è¯„è®ºæ•°: {comments}")
                
                video_info['rating'] = rating
                video_info['comments'] = comments
                video_info['comments_count'] = comments
                
                # è®¾ç½®è´¨é‡æ ‡ç­¾
                video_info['is_uncensored'] = (page_type == 'uncensored_ranking')
                video_info['is_hd'] = False  # æ’è¡Œæ¦œæ•°æ®é»˜è®¤ä¸æ ‡è®°ä¸ºé«˜æ¸…ï¼Œé¿å…å½±å“ç­›é€‰
                video_info['is_zh'] = False
                video_info['website'] = self.name
                
                videos.append(video_info)
                logger.info(f"âœ… æˆåŠŸè§£æè§†é¢‘: {num} - è¯„åˆ†: {rating} - è¯„è®º: {comments}")
                
            except Exception as e:
                logger.warning(f"è§£æ{page_type}è§†é¢‘ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
                continue
        
        logger.info(f"ğŸ¯ {page_type}ç¬¬{page}é¡µè§£æå®Œæˆ: æ€»æ•°{len(videos)}ä¸ª, æœ‰è¯„åˆ†{sum(1 for v in videos if v.get('rating'))}ä¸ª, æœ‰è¯„è®º{sum(1 for v in videos if v.get('comments', 0) > 0)}ä¸ª")
        return videos
    
    def _parse_uncensored_ranking_page(self, html, page):
        """è§£ææ— ç æ’è¡Œæ¦œé¡µé¢"""
        return self._parse_ranking_page(html, page, 'uncensored_ranking')

    def get_actors(self):
        """è·å–JavDBç½‘ç«™ä¸Šçš„çƒ­é—¨æ¼”å‘˜åˆ—è¡¨"""
        url = urljoin(self.host, '/actors')
        response = self._get(url)
        html_content = response.content
        
        # ä¿å­˜HTMLç”¨äºè°ƒè¯•
        try:
            with open("javdb_actors_debug.html", "wb") as f:
                f.write(html_content)
            logger.info(f"å·²ä¿å­˜æ¼”å‘˜åˆ—è¡¨é¡µé¢åˆ°javdb_actors_debug.htmlï¼Œé¡µé¢å¤§å°: {len(html_content)}å­—èŠ‚")
        except Exception as e:
            logger.error(f"ä¿å­˜è°ƒè¯•HTMLå¤±è´¥: {str(e)}")
        
        html = etree.HTML(html_content, parser=etree.HTMLParser(encoding='utf-8'))
        
        # è®°å½•XPathæŸ¥è¯¢ç»“æœ
        actors_list = html.xpath('//div[contains(@class, "actors-list")]')
        logger.info(f"æ‰¾åˆ°actors-listå…ƒç´ : {len(actors_list)}ä¸ª")
        
        if not actors_list:
            # ç½‘ç«™å¯èƒ½éœ€è¦ç™»å½•ï¼Œæˆ–è€…é¡µé¢ç»“æ„å˜åŒ–
            logger.warning("æœªæ‰¾åˆ°actors-listå…ƒç´ ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ¼”å‘˜å…ƒç´ ")
            
            # å°è¯•å…¶ä»–å¯èƒ½çš„XPath
            alt_actors = html.xpath('//div[contains(@class, "actor-box")]') or html.xpath('//a[contains(@href, "/actors/")]')
            logger.info(f"å°è¯•æ›¿ä»£æ–¹å¼æ‰¾åˆ°æ¼”å‘˜å…ƒç´ : {len(alt_actors)}ä¸ª")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å…ƒç´ ï¼Œå¯èƒ½æ˜¯ç½‘ç«™éœ€è¦ç™»å½•
            if not alt_actors:
                logger.error("æ— æ³•æ‰¾åˆ°ä»»ä½•æ¼”å‘˜å…ƒç´ ï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–ç½‘ç«™ç»“æ„å·²æ›´æ”¹")
                
                # è®°å½•ä¸€äº›é¡µé¢åŸºæœ¬ä¿¡æ¯ä»¥å¸®åŠ©è°ƒè¯•
                title = html.xpath('//title/text()')
                logger.info(f"é¡µé¢æ ‡é¢˜: {title}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•è¡¨å•
                login_form = html.xpath('//form[contains(@action, "login") or contains(@action, "sign_in")]')
                if login_form:
                    logger.error("æ£€æµ‹åˆ°ç™»å½•è¡¨å•ï¼Œç½‘ç«™å¯èƒ½éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®æ¼”å‘˜åˆ—è¡¨")
                
                return []
        
        result = []
        # å…ˆå°è¯•æ ‡å‡†è·¯å¾„
        actors = html.xpath('//div[contains(@class, "actors-list")]/div/a')
        
        if not actors:
            # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
            actors = html.xpath('//a[contains(@href, "/actors/")]')
            logger.info(f"ä½¿ç”¨æ›¿ä»£XPathæ‰¾åˆ°æ¼”å‘˜: {len(actors)}ä¸ª")
        
        logger.info(f"æ‰¾åˆ°æ¼”å‘˜å…ƒç´ : {len(actors)}ä¸ª")
        
        for actor in actors:
            try:
                actor_url = actor.get('href')
                if not actor_url or 'actors' not in actor_url:
                    continue
                
                actor_code = actor_url.split("/")[-1]
                
                # å°è¯•ä¸åŒæ–¹å¼æŸ¥æ‰¾æ¼”å‘˜åç§°
                name_el = actor.xpath('./div[@class="name"]')
                if name_el:
                    actor_name = name_el[0].text.strip()
                else:
                    # å°è¯•å…¶ä»–å¯èƒ½çš„æ–¹å¼è·å–åç§°
                    name_el = actor.xpath('.//text()')
                    if name_el and name_el[0].strip():
                        actor_name = name_el[0].strip()
                    else:
                        # å°è¯•ä»titleå±æ€§è·å–
                        actor_name = actor.get('title')
                        if not actor_name:
                            continue
                
                logger.info(f"æ‰¾åˆ°æ¼”å‘˜: {actor_name}, URL: {actor_url}")
                
                actor_avatar = urljoin(self.avatar_host, f'{actor_code[0:2].lower()}/{actor_code}.jpg')
                
                actor_info = VideoActor(name=actor_name, thumb=actor_avatar)
                result.append(actor_info)
            except Exception as e:
                logger.error(f"å¤„ç†æ¼”å‘˜å…ƒç´ æ—¶å‡ºé”™: {str(e)}")
                continue
        
        logger.info(f"æ€»å…±æ‰¾åˆ° {len(result)} ä¸ªæ¼”å‘˜")
        return result

    def search_actor(self, actor_name: str):
        """æœç´¢JavDBç½‘ç«™ä¸Šçš„æ¼”å‘˜"""
        url = urljoin(self.host, f'/search?q={actor_name}&f=actor')
        response = self._get(url)
        html = etree.HTML(response.content, parser=etree.HTMLParser(encoding='utf-8'))
        
        result = []
        
        # å°è¯•ä¸åŒçš„XPathè¡¨è¾¾å¼æ‰¾åˆ°æ¼”å‘˜åˆ—è¡¨
        actors = html.xpath('//a[contains(@href, "/actors/")]')
        
        for actor in actors:
            try:
                actor_url = actor.get('href')
                if not actor_url or 'actors' not in actor_url:
                    continue
                    
                actor_code = actor_url.split("/")[-1]
                
                # æŸ¥æ‰¾æ¼”å‘˜åç§°
                name = None
                name_el = actor.xpath('./strong') or actor.xpath('.//strong')
                if name_el:
                    name = name_el[0].text.strip()
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°æ ‡ç­¾ï¼Œå°è¯•ä½¿ç”¨å…ƒç´ è‡ªèº«çš„æ–‡æœ¬
                    name = actor.text.strip() if actor.text else None
                
                if not name:
                    # å¦‚æœè¿˜æ‰¾ä¸åˆ°åç§°ï¼Œå¯èƒ½æ¼”å‘˜åç§°åœ¨titleå±æ€§ä¸­
                    name = actor.get('title')
                    if name:
                        # å¦‚æœtitleåŒ…å«å¤šä¸ªåç§°ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œå–ç¬¬ä¸€ä¸ª
                        name = name.split(',')[0].strip()
                
                if not name:
                    continue
                
                # æ„é€ å¤´åƒURL
                actor_avatar = urljoin(self.avatar_host, f'{actor_code[0:2].lower()}/{actor_code}.jpg')
                
                actor_info = VideoActor(name=name, thumb=actor_avatar)
                result.append(actor_info)
            except Exception as e:
                continue
            
        return result
        
    def get_actor_videos(self, actor_url: str):
        """è·å–æ¼”å‘˜çš„æ‰€æœ‰è§†é¢‘ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥ä»æ¼”å‘˜é¡µé¢æå–æ‰€æœ‰ä¿¡æ¯"""
        import logging
        import re
        logger = logging.getLogger('spider')
        
        # å¤„ç†ä¸åŒæ ¼å¼çš„actor_urlè¾“å…¥
        if not actor_url.startswith(self.host):
            if not actor_url.startswith('http'):
                # å¦‚æœæä¾›çš„æ˜¯æ¼”å‘˜åç§°è€Œä¸æ˜¯URLï¼Œå…ˆæœç´¢è·å–æ¼”å‘˜ä¿¡æ¯
                actors = self.search_actor(actor_url)
                if not actors:
                    logger.info(f"æœªæ‰¾åˆ°æ¼”å‘˜: {actor_url}")
                    return []
                    
                # æ‰¾åˆ°æœ€åŒ¹é…çš„æ¼”å‘˜
                actor_match = None
                for actor in actors:
                    if actor.name.lower() == actor_url.lower() or actor_url.lower() in actor.name.lower():
                        actor_match = actor
                        break
                
                if not actor_match:
                    # æ’é™¤æ‰æœ‰ç¢¼ã€ç„¡ç¢¼ã€æ­ç¾ç­‰ç±»åˆ«æ ‡ç­¾
                    filtered_actors = [a for a in actors if a.name not in ['æœ‰ç¢¼', 'ç„¡ç¢¼', 'æ­ç¾']]
                    if filtered_actors:
                        actor_match = filtered_actors[0]
                    elif actors:
                        actor_match = actors[0]
                    else:
                        logger.error(f"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„æ¼”å‘˜: {actor_url}")
                        return []
                
                logger.info(f"é€‰æ‹©æ¼”å‘˜: {actor_match.name}")
                
                # ä»thumbä¸­æå–actor_id
                thumb_url = actor_match.thumb
                actor_code = thumb_url.split('/')[-1].split('.')[0]
                actor_url = urljoin(self.host, f'/actors/{actor_code}')
            elif '/actors/' not in actor_url:
                return []
            else:
                actor_url = urljoin(self.host, actor_url)
        
        logger.info(f"è®¿é—®æ¼”å‘˜ä½œå“é¡µ: {actor_url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Referer': self.host,
        }
        
        original_headers = self.session.headers.copy()
        self.session.headers.update(headers)
        
        try:
            # è®¿é—®æ¼”å‘˜é¡µé¢
            response = self._get(actor_url)
            html = etree.HTML(response.content, parser=etree.HTMLParser(encoding='utf-8'))
            
            result = []
            
            # ä»æ¼”å‘˜é¡µé¢ç›´æ¥æå–ä½œå“ä¿¡æ¯ï¼ˆåŒ…å«è¯„åˆ†ã€è¯„è®ºæ•°ã€æ—¥æœŸç­‰ï¼‰
            movie_boxes = html.xpath('//a[@class="box"]')
            logger.info(f"ä»æ¼”å‘˜é¡µé¢æ‰¾åˆ° {len(movie_boxes)} ä¸ªä½œå“")
            
            for box in movie_boxes:
                try:
                    item = JavDBRanking()
                    
                    # æå–è§†é¢‘URL
                    video_url = box.get('href')
                    if not video_url.startswith('http'):
                        video_url = urljoin(self.host, video_url)
                    item.url = video_url
                    
                    # æå–ç•ªå·å’Œæ ‡é¢˜
                    title_element = box.xpath('.//div[contains(@class, "video-title")]')
                    if title_element:
                        # æå–ç•ªå·
                        num_element = title_element[0].xpath('./strong/text()')
                        if num_element:
                            item.num = num_element[0].strip()
                        
                        # æå–å®Œæ•´æ ‡é¢˜
                        full_title = title_element[0].xpath('string(.)')
                        if full_title:
                            item.title = full_title.strip()
                    
                    # æå–å°é¢
                    cover_element = box.xpath('.//img[@loading="lazy"]')
                    if cover_element:
                        cover_url = cover_element[0].get('src')
                        if cover_url and not cover_url.startswith('http'):
                            cover_url = 'https:' + cover_url if cover_url.startswith('//') else urljoin(self.host, cover_url)
                        item.cover = cover_url
                    
                    # ç›´æ¥ä»æ¼”å‘˜é¡µé¢æå–è¯„åˆ†å’Œè¯„è®ºæ•°ä¿¡æ¯
                    score_element = box.xpath('.//div[contains(@class, "score")]//span[@class="value"]/text()')
                    if score_element:
                        score_text = score_element[0]
                        # è§£æè¯„åˆ†
                        score_match = re.search(r'(\d+\.\d+)åˆ†', score_text)
                        if score_match:
                            try:
                                rating_value = float(score_match.group(1))
                                item.rank = rating_value  # å‰ç«¯ä½¿ç”¨
                                item.rating = str(rating_value)  # æ¼”å‘˜è®¢é˜…ä½¿ç”¨
                            except:
                                pass
                        
                        # è§£æè¯„è®ºæ•°
                        count_match = re.search(r'ç”±(\d+)äººè©•åƒ¹', score_text)
                        if count_match:
                            try:
                                comments_value = int(count_match.group(1))
                                item.rank_count = comments_value
                            except:
                                pass
                    
                    # æ£€æŸ¥ä¸­æ–‡å­—å¹•å’Œæ— ç æ ‡ç­¾
                    cnsub_element = box.xpath('.//span[contains(@class, "cnsub")]')
                    item.isZh = len(cnsub_element) > 0
                    
                    uncensored_element = box.xpath('.//span[contains(@class, "uncensored")]')
                    item.is_uncensored = len(uncensored_element) > 0
                    
                    # æå–å‘å¸ƒæ—¥æœŸ
                    date_element = box.xpath('.//div[contains(@class, "meta")]/text()')
                    if date_element and len(date_element) > 0:
                        date_text = date_element[0].strip()
                        try:
                            # è§£ææ—¥æœŸæ ¼å¼ YYYY-MM-DD
                            if re.match(r'\d{4}-\d{2}-\d{2}', date_text):
                                item.publish_date = datetime.strptime(date_text, "%Y-%m-%d").date()
                        except:
                            pass
                    
                    # åªæœ‰æœ‰ç•ªå·çš„æ¡ç›®æ‰æ·»åŠ åˆ°ç»“æœä¸­
                    if item.num:
                        result.append(item)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†ä½œå“æ¡ç›®æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            logger.info(f"æˆåŠŸæå–åˆ° {len(result)} ä¸ªä½œå“ä¿¡æ¯ï¼ŒåŒ…å«è¯„åˆ†ã€è¯„è®ºæ•°ã€æ—¥æœŸç­‰å®Œæ•´ä¿¡æ¯")
            
            # è¾“å‡ºå‰3ä¸ªä½œå“çš„è¯¦ç»†ä¿¡æ¯ç”¨äºè°ƒè¯•
            for i, item in enumerate(result[:3]):
                logger.info(f"ä½œå“ {i+1}: {item.num} - è¯„åˆ†: {item.rank}/{item.rating} - è¯„è®º: {item.rank_count} - æ—¥æœŸ: {item.publish_date}")
            
            return result
            
        finally:
            # æ¢å¤åŸå§‹å¤´ä¿¡æ¯
            self.session.headers = original_headers
